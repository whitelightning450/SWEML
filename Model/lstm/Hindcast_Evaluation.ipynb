{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Performance Evaluation \n",
    "\n",
    "\n",
    "\n",
    "### Evaluation Methodology\n",
    "Model execution produces a 1-km grid SWE estimates across the Sierra Nevada Mountains and we can use the Standardized Snow Water Equivalent Evaluation tool (SSWEET) to perform a comphrehensive model evaluation against NASA ASO and snow course surveys.\n",
    "<img align = 'right' src=\"./Images/RMSE_R2.JPG\" alt = 'image' width = '200'/>\n",
    "\n",
    "SSWEET uses standard model evaluation metrics to guage model performance (i.e., Percent Bias (PBias, Coefficient of Determination (R2), Kling-Gupta Efficiency (KGE), and root mean squared error (RMSE)) and includes several methods to investitage the impacts of temporal-spatial characteristics influence model skill, ultimately to help refine model skill.\n",
    "A deep read through [The Abuse of Popular Performance Metrics in Hydrological Modeling](https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020WR029001) will provide more insight into model evaluation metrics and how to interpret their results related to model performance.\n",
    "\n",
    "PBias is a metric communicating the average tendency of the simulated values to be larger or smaller than their observed ones. \n",
    "The optimal value of PBias is 0.0, with low-magnitude values indicating accurate model simulation.\n",
    "\n",
    "The coefficient of determination (R<sup>2</sup>) is a unitless measurement of the proportion of explained variance of the target variable by the model.\n",
    "A maximum R<sup>2</sup> score of 1.0 indicates the predictor variables explain 100 percent of the variation in the target. \n",
    "A greater R<sup>2</sup> and lower RMSE represent better model predictive performance. \n",
    "\n",
    "KGE is simply the Euclidean distance computed using the coordinates of bias, standard deviation, and correlation. \n",
    "Similar to the coefficient of determination, values closer to 1 indicate greater model skill and due to the calculation of KGE, it will be lower than the bias, standard deviation, and correlation.\n",
    "\n",
    "RMSE is the quadratic mean of the differences between the observation and predictions, or residuals. \n",
    "RMSE aggregate the magnitudes of the residuals for all data points into a single measure of average model predictive power, with RMSE communicating the accuracy of the model.\n",
    "Note, RMSE is scale dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Preliminary Model Evaluation\n",
    "Preliminary model evaluation is on the 25% held out testing data, where the known previous SWE values from NASA ASO flights support the previous SWE feature during model inference.\n",
    "The following evaulation uses SSWEET to analyze model performance and statistically benchmark improvements compared to the current NSM.\n",
    "\n",
    "### Model Training/Testing influences and Bias on Model Performance.\n",
    "\n",
    "The model training/testing partitioning methodology has a strong influence on model performance and the goal of model evaluation.\n",
    "The objective of the modeling effort was to examine the spatial extrapolation capacity of the model from selected monitoring stations to the overall region, best suited to a 75/25% training/testing split, respectively.\n",
    "\n",
    "While it is critical to address the strong serial correlation in SWE accumulation and melt throughout the season, the high correlation between weeks has the potential to inflate model skill when using a 75/25% training/testing split due to the previous SWE feature being known.\n",
    "An assessment of the operational capacity of the model is different than assessing the ability to extrapolate regional SWE from in-situ monitoring stations and is the reason WY 2019 was held out of the training data as it will form the final model validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from shared_scripts import Hindcast_Initialization\n",
    "from shared_scripts import SSWEET\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Set working directories\n",
    "cwd = os.getcwd() \n",
    "datapath = f\"{os.path.expanduser('~')}/SWEML\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Load your model data\n",
    "\n",
    "As long as model development followed the prescribed template, the SSWEET.load_Predictions() function will correctly load and process model predictions and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get datetime and corresponding background information to evaluate hindcast\n",
    "\n",
    "new_year = '2019'\n",
    "threshold = '20.0'\n",
    "Region_list = ['N_Sierras','S_Sierras_High', 'S_Sierras_Low']\n",
    "model = 'LSTM'\n",
    "\n",
    "datelist = Hindcast_Initialization.Hindcast_Initialization(cwd, datapath, new_year, threshold, Region_list)\n",
    "EvalDF = Hindcast_Initialization.HindCast_DataProcess(datelist,Region_list,cwd, datapath, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Parity Plot\n",
    "\n",
    "A parity plot is a scatterplot that compares a set of model estimates against benchmark data, i.e., the observations.\n",
    "Each point has coordinates (x, y), where x is a benchmark value and y is the corresponding value from the model.\n",
    "A parity plot is often the first visualization to investigate the skill of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.parityplot(EvalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "\n",
    "## More in-depth model evaluation.\n",
    "We want to evaluate the model over the course of seasonal snow accumulation and melt, at differnt elevation bands, and spatially.\n",
    "\n",
    "\n",
    "Using SSWEET, the Model_Vs() function supports an in-depth evaluation of multiple model skill influencing components to conduct a robust and comprehensive evaluation of a model.\n",
    "The Model_Vs() function takes the following inputs: Model_Vs(RegionTest,variable,Model Output, datapath), where\n",
    "* RegionTest is the prediction dataframe saved from the training notebook.\n",
    "* variable is the variable of interest. Currently supported variables include Water Year Week (WYWeek), Elevation (elevation_m), Previous SWE Estimate (prev_SWE), Latitude (Lat), and Northness (northness), and Error due to Previous SWE Estimate (prev_SWE_error).\n",
    "* Model Output refers to one of three model outputs or processed model outputs including: Predictions (Prediction), Error (Error) which is the physical difference between each prediction and observation, and Percent Error (Percent_Error) which is Error divided by the Observation and multiplied by 100%.\n",
    "\n",
    "The following examples demonstrate the utilty of the SSWEET and the Model_Vs() function.\n",
    "\n",
    "\n",
    "### Error over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'WYWeek', 'Error', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Error compared to elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'elevation_m', 'Error', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Percent Error compared to elevation\n",
    "*note, error greater than |100%| is adjusted to |100%| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'elevation_m', 'Percent_Error', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Model error  compared to northness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'northness', 'Error', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Interactive Spatial Plotting\n",
    "While fitures support evaluation, spatially examining model performance has several benefits adn the Map_Plot_Eval function() supports a spatial method to evaluate model skill.\n",
    "The Map_Plot_Eval(datapath, RegionTest, yaxis, error_metric) function supports three different evaluation metrics: 'KGE', 'cm', or '%'.\n",
    "The physical error (cm) illustrates how close the predictions are to the observed, the mean percentage error (%) illustrates the perentagewise prediction accuracy, and KGE (KGE) illustrates the mean, variance and correlation on model performance, however, it is only useful for sites with multiple predictions. \n",
    "\n",
    "### Spatially evaluating the Model via KGE\n",
    "Selecting the prediction where there was a previous SWE value demonstrates significanly increased KGE. Note, the blue icons are the SNOTEL sites used to inform predictions and are more visible now. \n",
    "When running interactively, clicking on the SNOTEL icon shows the SNOTEL site information and allwos for an investigation of errors stemming from the proximity of the prediction locations to in situ observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folium plot\n",
    "SSWEET.Map_Plot_Eval(datapath, EvalDF,'SWE (cm)', 'KGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Spatially evaluating the Model via error in percentage from the observed\n",
    "\n",
    "We can now see that the model can spatially extrapolate and that there is a significant need for improved temporal resolution data. \n",
    "For this case, KGE does not provide a useful method to evaluate the model.\n",
    "Alternatively, the Map_Plot_Eval() function supports percent error (%) and physical error (cm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folium plot\n",
    "SSWEET.Map_Plot_Eval(datapath, EvalDF,'SWE (cm)', '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Spatially evaluating the Model via error in cm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folium plot\n",
    "SSWEET.Map_Plot_Eval(datapath, EvalDF,'SWE (cm)', 'cm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Other examples of using SSWEET to investigate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'prev_SWE_error', 'Error', datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'Lat', 'Error', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Exploring the Prediction results\n",
    "\n",
    "While model error is a critical metric to gauge model skil, simple checks on model predictions can support the use of a model or high areas for improvement.\n",
    "For example, it montane regions, SWE generally increases in elevation up to the apline (or above treeline). \n",
    "Capturing this trend is important and a critical element of model evaluation.\n",
    "Once above treeline (~2,900 m in the Sierra Nevada mountains), SWE distribution is significantly affected by overall and microclimate wind speed and direction, leaving some high altitude regions completly bare and others with large amounts of SWE. \n",
    "Note, many of these aspects are scale dependent (e.g., a 50 m, 250 m, 1 km, and 25 km model will vary significantly due to varying geopatial characteristics and key hydrological processes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'elevation_m', 'Prediction', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Effective modeling of elevation gradients leads to a realistic modeled SWE across heterogeneous terrain. \n",
    "For example, the  higher elevation bands display greater modeled SWE values compared to lower elevations, and the highest elevation bands display less SWE reflecting exposed terrain subject to wind transport and snow redistribution.\n",
    "This is indicative that the model is capable of generalizing topographical and geographical characteristics to effectivley translate elevation gradients on SWE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'northness', 'Prediction', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "While the model uses northness as a feature to represent the average slope and aspect of the 1-km grid, overlaying the predictions on complex topography indicates a need for high resolution prediction.\n",
    "Examination of the 1-km resolution indicates it is too coarse to capture rapid topographical changes common to montane environments, but does indicate an increas in northness lead to an increase in SWE.\n",
    "Investigating the distribution of observation/predictions indicates there is a need to a larger, more comprehensive training dataset to capture key influences on SWE distribution in montane environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSWEET.Model_Vs(EvalDF,'Lat', 'Prediction', datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "from datetime import date, datetime, timedelta\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import contextily as cx #contextily-1.4.0 mercantile-1.2.1\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import contextlib\n",
    "from PIL import Image\n",
    "from IPython.display import Image as ImageShow\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from progressbar import ProgressBar\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"SWEML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "BUCKET_NAME = 'national-snow-model'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send all prediction dfs to AWS\n",
    "AWSpath = \"Hold_Out_Year/\"\n",
    "\n",
    "def DF_to_AWS(modelname, folderpath, AWSpath, type):\n",
    " \n",
    "    files = []\n",
    "    for file in os.listdir(folderpath):\n",
    "        if file.endswith(type):\n",
    "            files.append(file)\n",
    "    print(files)\n",
    "\n",
    "    #Load and push to AWS\n",
    "    pbar = ProgressBar()\n",
    "    print('Pushing files to AWS')\n",
    "    for file in pbar(files):\n",
    "        filepath = f\"{folderpath}{file}\"\n",
    "        S3.meta.client.upload_file(Filename= filepath, Bucket=BUCKET_NAME, Key=f\"{modelname}/{AWSpath}{file}\")\n",
    "\n",
    "#send all prediction dfs to AWS\n",
    "modelname = 'LSTM'\n",
    "AWSpath = \"Hold_Out_Year/\"\n",
    "folderpath = f\"Predictions/{AWSpath}Predictions/\"\n",
    "type = '.pkl'\n",
    "\n",
    "Hindcast_Initialization.DF_to_AWS(modelname, folderpath, AWSpath, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from shared_scripts import Hindcast_Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
