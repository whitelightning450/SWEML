{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f45d1d3-2ce5-487c-8d89-a2e0ec3870ec",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "<img align = 'center' src=\"./Images/ML_SWE.jpg\" alt = 'image' width = '1000'/>\n",
    "\n",
    "\n",
    "# Add AORC Precipitation to Prediction Datasets\n",
    "\n",
    "- Tony Castronova <acastronova@cuahsi.org>\n",
    "- Irene Garousi-Nejad <igarousi@cuahsi.org>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e9376-f3b9-4b11-a32f-d9af15fde2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install dask[distributed] zarr xarray pandas s3fs kerchunk scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c353a2-226b-4dfb-8db4-c2fbcadb2bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "import zarr\n",
    "import numpy\n",
    "import xarray\n",
    "import pyproj\n",
    "import pandas\n",
    "from s3fs import S3FileSystem\n",
    "from dask.distributed import Client, progress\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "import dask.bag as db  \n",
    "\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import aorc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79279a53-22ae-40cc-a05c-67188f18ccc4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Initiate the Dask client. This will enable us to parallelize our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005eb820-57c2-4ad6-81e8-26ed37d4f075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use a try accept loop so we only instantiate the client\n",
    "# if it doesn't already exist.\n",
    "try:\n",
    "    print(client.dashboard_link)\n",
    "except:    \n",
    "    # The client should be customized to your workstation resources.\n",
    "    client = Client(n_workers=20)\n",
    "    print(client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4cfb0-c8e4-4843-85f8-294cad5d22df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def extract_dask(search_points, all_points, final_shape):\n",
    "    index = pairwise_distances_argmin(X=search_points,\n",
    "                                      Y=all_points)\n",
    "    i0, j0 = numpy.unravel_index(index, (final_shape))\n",
    "    return(j0, i0)\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def get_data_dask(i_locs, j_locs, year='2010', month='01', day='01'):\n",
    "    ds = aorc1.load_aorc_dataset(year, month, day)\n",
    "    precip = ds.isel(x=i_locs, y=j_locs).squeeze().RAINRATE\n",
    "    \n",
    "    \n",
    "    with open(f'{year}{month}{day}.pkl', 'wb') as f:\n",
    "        pickle.dump(precip.values, f)\n",
    "    \n",
    "    return datetime(int(year), int(month), int(day)),\n",
    "\n",
    "\n",
    "def get_data_daskbag(args):\n",
    "    i_locs = args[0]\n",
    "    j_locs = args[1]\n",
    "    dt = args[2]\n",
    "    path = args[3]\n",
    "    \n",
    "    # get the date parts\n",
    "    month = f'{dt.month:02}'    \n",
    "    day = f'{dt.day:02}'\n",
    "    year = f'{dt.year:04}'\n",
    "    \n",
    "    # if os.path.exists(f'{path}/{year}{month}{day}.csv'):\n",
    "    #     return f'{path}/{year}{month}{day}.csv'\n",
    "\n",
    "    ds = aorc1.load_aorc_dataset(year, month, day)\n",
    "    \n",
    "    precip = ds.isel(x=i_locs, y=j_locs) #.squeeze().RAINRATE\n",
    "    precip = precip.RAINRATE.groupby('time.dayofyear').sum() * 24 * 3600\n",
    "    pcp_df = precip.to_dataframe().reset_index()\n",
    "    \n",
    "    pcp_df['date'] = dt\n",
    "    pcp_df = pcp_df[['lat', 'lon', 'RAINRATE', 'date']]\n",
    "    pcp_df.lat = round(pcp_df.lat, 6)  # rounding so they match later \n",
    "    pcp_df.lon = round(pcp_df.lon, 6)  # rounding so they match later\n",
    "    pcp_df.rename(columns={'RAINRATE': 'RAINRATE [mm]'}, inplace=True)\n",
    "    pcp_df.set_index('date', inplace=True)\n",
    "    pcp_df.to_csv(f'{path}/{year}{month}{day}.csv')\n",
    "    \n",
    "    return f'{path}/{year}{month}{day}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bb9cd-cf11-4089-accb-fd50b1a99230",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load AORC V1.0 from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331f6ce-d72e-4710-9095-208157c260e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get all of the lat/lon locations in AORC\n",
    "\n",
    "ds = aorc1.load_aorc_dataset('2010', '01', '01')\n",
    "all_pts = numpy.c_[ds['lon'].values.ravel(), ds['lat'].values.ravel()]\n",
    "all_lats = ds['lat'].values\n",
    "all_lons = ds['lon'].values\n",
    "final_shape = ds['lon'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbfa9d-c62f-42cf-929f-dddfed2ee34e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_path = \"/home/jovyan/Snow-Extrapolation/data/RegionTrain_SCA.pkl\"\n",
    "with open(training_path, 'rb') as f:\n",
    "    region_train = pickle.load(f)\n",
    "\n",
    "pts = []\n",
    "locs = []\n",
    "for key in region_train.keys():\n",
    "    region_train[key]['pt'] = list(zip(region_train[key].Long, region_train[key].Lat))\n",
    "    locs.extend([*region_train[key].pt.unique()])\n",
    "\n",
    "print(f'Number of training points = {len(locs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f09d5b-14cb-429b-970a-747ffe626b93",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Collect AORC Precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77383ae6-da6a-41b3-90a6-54cea57353df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_path = 'Predictions/Hold_Out_Year/Predictions/'\n",
    "files = glob(f'{prediction_path}/*.pkl')\n",
    "\n",
    "with open(files[0], 'rb') as f:\n",
    "    dat = pickle.load(f)\n",
    "    \n",
    "pts = []\n",
    "locs = []\n",
    "for key in ['N_Sierras', 'S_Sierras_Low', 'S_Sierras_High']:\n",
    "    dat[key]['pt'] = list(zip(dat[key].Long, dat[key].Lat))\n",
    "    locs.extend([*dat[key].pt.unique()])\n",
    "\n",
    "print(f'Number of testing points = {len(locs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fd94c-b017-42ed-b0e1-9355b2cb5230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# batch index collection using dask\n",
    "number_of_groups = 100\n",
    "pt_groups = numpy.array_split(numpy.array(locs), number_of_groups)\n",
    "\n",
    "print('scattering...', end='', flush=True)\n",
    "all_pts_scattered = client.scatter(all_pts)\n",
    "print('done')\n",
    "\n",
    "futures = []\n",
    "for grp in pt_groups:\n",
    "    futures.append(extract_dask(grp, all_pts_scattered, final_shape)) \n",
    "\n",
    "print('finding aorc lats/lons...', end='', flush=True)\n",
    "results = dask.compute(futures)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b9de0-8e14-4bf1-a1b7-1c7b5afe6fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put the x,y coordinates for the matching cells into lists\n",
    "i_locs = []\n",
    "j_locs = []\n",
    "for grp in results[0]:\n",
    "    num_elements = len(grp[0])\n",
    "    for idx in range(0, num_elements):\n",
    "        i_locs.append(grp[0][idx])\n",
    "        j_locs.append(grp[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d75f1-05e1-4686-8a14-e46310bf3d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create mapping between nsm lat/lon and aorc i,j \n",
    "data = []\n",
    "for i in range(0, len(i_locs)):\n",
    "    aorc_lat = round(all_lats[j_locs[i]][i_locs[i]], 6)\n",
    "    aorc_lon = round(all_lons[j_locs[i]][i_locs[i]], 6)\n",
    "    data.append([locs[i][0], locs[i][1], i_locs[i], j_locs[i], aorc_lon, aorc_lat])\n",
    "\n",
    "headers = ['nsm_long', 'nsm_lat', 'aorc_i', 'aorc_j', 'aorc_lon', 'aorc_lat'] \n",
    "\n",
    "loc_map = pandas.DataFrame(columns=headers, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fd48f-6e60-4192-92b5-35b8a33ea956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "if not os.path.exists('data_preds'):\n",
    "    os.mkdir('data_preds')\n",
    "    \n",
    "# batch variable collection\n",
    "t = datetime(2018,9,21)\n",
    "et = datetime(2019,6,30)\n",
    "\n",
    "# create dataarrays for subsetting aorc pointwise\n",
    "ind_x = xarray.DataArray(i_locs, dims=[\"pt\"])\n",
    "ind_y = xarray.DataArray(j_locs, dims=[\"pt\"])\n",
    "\n",
    "input_params = []\n",
    "while t <= et:\n",
    "    if not os.path.exists(f\"data_preds/{t.strftime('%Y%m%d')}.csv\"):\n",
    "        input_params.append([ind_x, ind_y, t, 'data_preds'])\n",
    "    t += timedelta(days=1)\n",
    "    \n",
    "b = db.from_sequence(input_params, npartitions=25)\n",
    "b = b.map(get_data_daskbag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd9b70-87af-4b44-be2c-dc8412bf4d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = b.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2a815-b334-41bf-a40a-4ad75009c67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we don't need dask anymore, so release all memory consumed by it\n",
    "client.cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cdef6-02ec-4ea9-85c6-bedb769b9a2e",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Summarize Precipitation and Save to Prediction Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7cc027-24e2-45ce-8da5-9e5fdf2858f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load aorc precip from CSVs into a dataframe\n",
    "#df_precip = pandas.concat([pandas.read_csv(f) for f in results])\n",
    "\n",
    "df_precip = pandas.concat([pandas.read_csv(f) for f in glob('data_preds/*.csv')])\n",
    "\n",
    "df_precip.date = pandas.to_datetime(df_precip.date)\n",
    "df_precip.set_index('date', inplace=True)\n",
    "\n",
    "# compute weekly precip\n",
    "df_precip = df_precip.groupby(['lat', 'lon', pandas.Grouper(freq='W-Tue')]).sum().reset_index()\n",
    "\n",
    "# rename columns\n",
    "df_precip.rename(columns={'RAINRATE [mm]':'aorc-precip-weekly-mm', 'lat':'aorc_lat', 'lon':'aorc_lon'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e3b43-d510-443d-b95e-4fc52976091f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding aorc lat/lon so we can join into the training dataset\n",
    "df_precip = pandas.merge(df_precip, loc_map, how='left',\n",
    "             left_on=['aorc_lon', 'aorc_lat'],\n",
    "             right_on=['aorc_lon', 'aorc_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e408e-6ea9-4bfb-bc4f-ec84a797e68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute shifted precip\n",
    "for idx, _ in df_precip.groupby(['nsm_long', 'nsm_lat']):\n",
    "    d = df_precip.loc[(df_precip.nsm_long == idx[0]) & (df_precip.nsm_lat == idx[1])]['aorc-precip-weekly-mm'].shift(1)\n",
    "    df_precip.loc[(df_precip.nsm_long == idx[0]) & (df_precip.nsm_lat == idx[1]), 'Prev_aorc-precip-weekly-mm'] = d \n",
    "\n",
    "## TODO reset accumulation every year\n",
    "# # compute cumulative precip \n",
    "# for idx, _ in df_precip.groupby(['nsm_long', 'nsm_lat']):\n",
    "#     d = df_precip.loc[(df_precip.nsm_long == idx[0]) & (df_precip.nsm_lat == idx[1])]['aorc-precip-weekly-mm'].cumsum()\n",
    "#     df_precip.loc[(df_precip.nsm_long == idx[0]) & (df_precip.nsm_lat == idx[1]), 'cum-precip-mm'] = d \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e050039-a55b-4a2e-b340-aa3c2a66dc89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0516b8d-2091-446e-9685-5fb1fa1246db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save results to the prediction files\n",
    "\n",
    "prediction_path = '/Predictions/Hold_Out_Year/Predictions/'\n",
    "files = glob(f'{prediction_path}/*.pkl')\n",
    "\n",
    "for file in files:\n",
    "    date = datetime.strptime(file.split('_')[-1].split('.')[0], '%Y-%m-%d')\n",
    "    with open(file, 'rb') as f:\n",
    "        regions_df = pickle.load(f)\n",
    "    \n",
    "    df_precip_by_date = df_precip.loc[df_precip.date == date]\n",
    "    \n",
    "    for key in ['N_Sierras', 'S_Sierras_Low', 'S_Sierras_High']:\n",
    "\n",
    "        region = regions_df[key]\n",
    "        region.reset_index(inplace=True)\n",
    "        region = pandas.merge(region, df_precip_by_date[['aorc-precip-weekly-mm', 'Prev_aorc-precip-weekly-mm', 'nsm_long', 'nsm_lat']],\n",
    "                              how='left', left_on=['Long', 'Lat'],\n",
    "                              right_on=['nsm_long', 'nsm_lat']).set_index('cell_id')\n",
    "        region.drop(['nsm_long', 'nsm_lat'], inplace=True, axis=1)\n",
    "        \n",
    "        # save this back to the training dataset\n",
    "        regions_df[key] = region\n",
    "        \n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(regions_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a033a-8633-4a57-bed4-8e5a67e3193b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
