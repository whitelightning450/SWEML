{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9582b19b-a266-4a82-9f9d-7d5ad1e3a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjohnson18/envs/SWEML_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-17 15:10:45.142863: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-17 15:10:45.142890: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "# Dataframe Packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Vector Packages\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# Raster Packages\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rioxarray.merge import merge_arrays\n",
    "import rasterstats as rs\n",
    "import osgeo\n",
    "from osgeo import gdalconst\n",
    "\n",
    "# Data Access Packages\n",
    "import earthaccess as ea\n",
    "import h5py\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from pystac_client import Client\n",
    "import richdem as rd\n",
    "import planetary_computer\n",
    "from planetary_computer import sign\n",
    "\n",
    "# General Packages\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from retrying import retry\n",
    "import fiona\n",
    "import re\n",
    "import s3fs\n",
    "\n",
    "#need to mamba install gdal, earthaccess \n",
    "#pip install pystac_client, richdem, planetary_computer, dask, distributed, retrying\n",
    "\n",
    "#connecting to AWS\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import os\n",
    "\n",
    "import NSIDC_Data\n",
    "'''\n",
    "To create .netrc file:\n",
    "import earthaccess\n",
    "earthaccess.login(persist=True)\n",
    "open file and change machine to https://urs.earthdata.nasa.gov\n",
    "\n",
    "'''\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"SWEML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'national-snow-model'\n",
    "#S3 = boto3.resource('S3', config=Config(signature_version=UNSIGNED))\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab6edbc-c701-42b3-89c9-4649ac4dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASODataTool:\n",
    "    def __init__(self, short_name, version, polygon='', filename_filter=''):\n",
    "        self.short_name = short_name\n",
    "        self.version = version\n",
    "        self.polygon = polygon\n",
    "        self.filename_filter = filename_filter\n",
    "        self.url_list = []\n",
    "        self.CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
    "        self.CMR_PAGE_SIZE = 2000\n",
    "        self.CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
    "                             '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
    "                             '&scroll=true&page_size={1}'.format(self.CMR_URL, self.CMR_PAGE_SIZE))\n",
    "\n",
    "    def cmr_search(self, time_start, time_end, bounding_box):\n",
    "        try:\n",
    "            if not self.url_list:\n",
    "                self.url_list = NSIDC_Data.cmr_search(\n",
    "                    self.short_name, self.version, time_start, time_end,\n",
    "                    bounding_box=self.bounding_box, polygon=self.polygon,\n",
    "                    filename_filter=self.filename_filter, quiet=False)\n",
    "            return self.url_list\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "\n",
    "    def cmr_download(self, directory):\n",
    "        dpath = f\"{HOME}/SWEML/data/NSMv2.0/data/ASO/{directory}\"\n",
    "        if not os.path.exists(dpath):\n",
    "            os.makedirs(dpath, exist_ok=True)\n",
    "\n",
    "        NSIDC_Data.cmr_download(self.url_list, dpath, False)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bounding_box(region):\n",
    "        try:\n",
    "            regions = pd.read_pickle(f\"{HOME}/SWEML/data/PreProcessed/RegionVal.pkl\")\n",
    "        except:\n",
    "            print('File not local, getting from AWS S3.')\n",
    "            key = f\"data/PreProcessed/RegionVal.pkl\"            \n",
    "            S3.meta.client.download_file(BUCKET_NAME, key,f\"{HOME}/SWEML/data/PreProcessed/RegionVal.pkl\")\n",
    "            regions = pd.read_pickle(f\"{HOME}/SWEML/data/PreProcessed/RegionVal.pkl\")\n",
    "\n",
    "\n",
    "        \n",
    "        superset = []\n",
    "\n",
    "        superset.append(regions[region])\n",
    "        superset = pd.concat(superset)\n",
    "        superset = gpd.GeoDataFrame(superset, geometry=gpd.points_from_xy(superset.Long, superset.Lat, crs=\"EPSG:4326\"))\n",
    "        bounding_box = list(superset.total_bounds)\n",
    "\n",
    "        return f\"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}\"\n",
    "\n",
    "class ASODownload(ASODataTool):\n",
    "    def __init__(self, short_name, version, polygon='', filename_filter=''):\n",
    "        super().__init__(short_name, version, polygon, filename_filter)\n",
    "        self.region_list =    [ 'N_Sierras',\n",
    "                                'S_Sierras',\n",
    "                                'Greater_Yellowstone',\n",
    "                                'N_Co_Rockies',\n",
    "                                'SW_Mont',\n",
    "                                'SW_Co_Rockies',\n",
    "                                'GBasin',\n",
    "                                'N_Wasatch',\n",
    "                                'N_Cascade',\n",
    "                                'S_Wasatch',\n",
    "                                'SW_Mtns',\n",
    "                                'E_WA_N_Id_W_Mont',\n",
    "                                'S_Wyoming',\n",
    "                                'SE_Co_Rockies',\n",
    "                                'Sawtooth',\n",
    "                                'Ca_Coast',\n",
    "                                'E_Or',\n",
    "                                'N_Yellowstone',\n",
    "                                'S_Cascade',\n",
    "                                'Wa_Coast',\n",
    "                                'Greater_Glacier',\n",
    "                                'Or_Coast'  ]\n",
    "\n",
    "    def select_region(self):\n",
    "        print(\"Select a region by entering its index:\")\n",
    "        for i, region in enumerate(self.region_list, start=1):\n",
    "            print(f\"{i}. {region}\")\n",
    "\n",
    "        try:\n",
    "            user_input = int(input(\"Enter the index of the region: \"))\n",
    "            if 1 <= user_input <= len(self.region_list):\n",
    "                selected_region = self.region_list[user_input - 1]\n",
    "                self.bounding_box = self.get_bounding_box(selected_region)\n",
    "                print(f\"You selected: {selected_region}\")\n",
    "                print(f\"Bounding Box: {self.bounding_box}\")\n",
    "            else:\n",
    "                print(\"Invalid index. Please select a valid index.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid index.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050667d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a region by entering its index:\n",
      "1. N_Sierras\n",
      "2. S_Sierras\n",
      "3. Greater_Yellowstone\n",
      "4. N_Co_Rockies\n",
      "5. SW_Mont\n",
      "6. SW_Co_Rockies\n",
      "7. GBasin\n",
      "8. N_Wasatch\n",
      "9. N_Cascade\n",
      "10. S_Wasatch\n",
      "11. SW_Mtns\n",
      "12. E_WA_N_Id_W_Mont\n",
      "13. S_Wyoming\n",
      "14. SE_Co_Rockies\n",
      "15. Sawtooth\n",
      "16. Ca_Coast\n",
      "17. E_Or\n",
      "18. N_Yellowstone\n",
      "19. S_Cascade\n",
      "20. Wa_Coast\n",
      "21. Greater_Glacier\n",
      "22. Or_Coast\n",
      "You selected: S_Sierras\n",
      "Bounding Box: -120.3763448720203,36.29256774541929,-118.292253412863,38.994985247736324\n",
      "Fetching file URLs in progress for None from 2013-04-02T00:00:00Z to 2019-07-19T23:59:59Z\n",
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=ASO_50M_SWE&version=001&version=01&version=1&temporal[]=2013-04-02T00:00:00Z,2019-07-19T23:59:59Z&bounding_box=-120.3763448720203,36.29256774541929,-118.292253412863,38.994985247736324\n",
      "\n",
      "Found 131 matches.\n",
      "['https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.04.03/ASO_50M_SWE_USCATB_20130403.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.04.03/ASO_50M_SWE_USCATB_20130403.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.04.29/ASO_50M_SWE_USCATB_20130429.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.04.29/ASO_50M_SWE_USCATB_20130429.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.05.03/ASO_50M_SWE_USCATB_20130503.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.05.03/ASO_50M_SWE_USCATB_20130503.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.05.25/ASO_50M_SWE_USCATB_20130525.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.05.25/ASO_50M_SWE_USCATB_20130525.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.06.01/ASO_50M_SWE_USCATB_20130601.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.06.01/ASO_50M_SWE_USCATB_20130601.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.06.08/ASO_50M_SWE_USCATB_20130608.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2013.06.08/ASO_50M_SWE_USCATB_20130608.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.03.20/ASO_50M_SWE_USCOUB_20140320.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.03.20/ASO_50M_SWE_USCOUB_20140320.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.03.23/ASO_50M_SWE_USCATB_20140323.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.03.23/ASO_50M_SWE_USCATB_20140323.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.03.24/ASO_50M_SWE_USCAMB_20140324.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.03.24/ASO_50M_SWE_USCAMB_20140324.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.06/ASO_50M_SWE_USCAMB_20140406.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.06/ASO_50M_SWE_USCAMB_20140406.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.07/ASO_50M_SWE_USCATB_20140407.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.07/ASO_50M_SWE_USCATB_20140407.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.14/ASO_50M_SWE_USCAMB_20140414.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.14/ASO_50M_SWE_USCAMB_20140414.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.20/ASO_50M_SWE_USCATB_20140420.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.20/ASO_50M_SWE_USCATB_20140420.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.23/ASO_50M_SWE_USCAMB_20140423.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.23/ASO_50M_SWE_USCAMB_20140423.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.28/ASO_50M_SWE_USCATB_20140428.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.28/ASO_50M_SWE_USCATB_20140428.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.29/ASO_50M_SWE_USCAMB_20140429.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.04.29/ASO_50M_SWE_USCAMB_20140429.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.02/ASO_50M_SWE_USCATB_20140502.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.02/ASO_50M_SWE_USCATB_20140502.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.03/ASO_50M_SWE_USCAMB_20140503.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.03/ASO_50M_SWE_USCAMB_20140503.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.11/ASO_50M_SWE_USCATB_20140511.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.11/ASO_50M_SWE_USCATB_20140511.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.12/ASO_50M_SWE_USCAMB_20140512.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.12/ASO_50M_SWE_USCAMB_20140512.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.17/ASO_50M_SWE_USCATB_20140517.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.17/ASO_50M_SWE_USCATB_20140517.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.27/ASO_50M_SWE_USCATB_20140527.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.27/ASO_50M_SWE_USCATB_20140527.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.31/ASO_50M_SWE_USCATB_20140531.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.05.31/ASO_50M_SWE_USCATB_20140531.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.06.05/ASO_50M_SWE_USCATB_20140605.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2014.06.05/ASO_50M_SWE_USCATB_20140605.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.02.17/ASO_50M_SWE_USCATB_20150217.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.02.17/ASO_50M_SWE_USCATB_20150217.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.03.05/ASO_50M_SWE_USCATB_20150305.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.03.05/ASO_50M_SWE_USCATB_20150305.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.03.25/ASO_50M_SWE_USCATB_20150325.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.03.25/ASO_50M_SWE_USCATB_20150325.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.03.26/ASO_50M_SWE_USCARC_20150326.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.03.26/ASO_50M_SWE_USCARC_20150326.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.03/ASO_50M_SWE_USCAKC_20150403.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.03/ASO_50M_SWE_USCAKC_20150403.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.03/ASO_50M_SWE_USCATB_20150403.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.03/ASO_50M_SWE_USCATB_20150403.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.09/ASO_50M_SWE_USCATB_20150409.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.09/ASO_50M_SWE_USCATB_20150409.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.12/ASO_50M_SWE_USCAKC_20150412.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.12/ASO_50M_SWE_USCAKC_20150412.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.15/ASO_50M_SWE_USCATB_20150415.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.15/ASO_50M_SWE_USCATB_20150415.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.26/ASO_50M_SWE_USCAKC_20150426.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.26/ASO_50M_SWE_USCAKC_20150426.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.27/ASO_50M_SWE_USCATB_20150427.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.27/ASO_50M_SWE_USCATB_20150427.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.28/ASO_50M_SWE_USCALB_20150428.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.28/ASO_50M_SWE_USCALB_20150428.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.28/ASO_50M_SWE_USCAMB_20150428.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.04.28/ASO_50M_SWE_USCAMB_20150428.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.03/ASO_50M_SWE_USCARC_20150503.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.03/ASO_50M_SWE_USCARC_20150503.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.27/ASO_50M_SWE_USCARC_20150527.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.27/ASO_50M_SWE_USCARC_20150527.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.28/ASO_50M_SWE_USCATB_20150528.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.28/ASO_50M_SWE_USCATB_20150528.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.31/ASO_50M_SWE_USCAKC_20150531.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.05.31/ASO_50M_SWE_USCAKC_20150531.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.06.08/ASO_50M_SWE_USCATB_20150608.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.06.08/ASO_50M_SWE_USCATB_20150608.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.06.09/ASO_50M_SWE_USCARC_20150609.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2015.06.09/ASO_50M_SWE_USCARC_20150609.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.03.26/ASO_50M_SWE_USCATB_20160326.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.03.26/ASO_50M_SWE_USCATB_20160326.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.01/ASO_50M_SWE_USCACE_20160401.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.01/ASO_50M_SWE_USCACE_20160401.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.01/ASO_50M_SWE_USCATB_20160401.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.01/ASO_50M_SWE_USCATB_20160401.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.07/ASO_50M_SWE_USCACE_20160407.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.07/ASO_50M_SWE_USCACE_20160407.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.07/ASO_50M_SWE_USCATB_20160407.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.07/ASO_50M_SWE_USCATB_20160407.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.16/ASO_50M_SWE_USCACE_20160416.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.16/ASO_50M_SWE_USCACE_20160416.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.16/ASO_50M_SWE_USCATB_20160416.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.16/ASO_50M_SWE_USCATB_20160416.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.26/ASO_50M_SWE_USCACE_20160426.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.26/ASO_50M_SWE_USCACE_20160426.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.26/ASO_50M_SWE_USCATB_20160426.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.04.26/ASO_50M_SWE_USCATB_20160426.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.09/ASO_50M_SWE_USCACE_20160509.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.09/ASO_50M_SWE_USCACE_20160509.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.09/ASO_50M_SWE_USCALB_20160509.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.09/ASO_50M_SWE_USCALB_20160509.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.09/ASO_50M_SWE_USCATB_20160509.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.09/ASO_50M_SWE_USCATB_20160509.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.27/ASO_50M_SWE_USCATB_20160527.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.05.27/ASO_50M_SWE_USCATB_20160527.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.07/ASO_50M_SWE_USCALB_20160607.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.07/ASO_50M_SWE_USCALB_20160607.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.14/ASO_50M_SWE_USCALB_20160614.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.14/ASO_50M_SWE_USCALB_20160614.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.21/ASO_50M_SWE_USCALB_20160621.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.21/ASO_50M_SWE_USCALB_20160621.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.26/ASO_50M_SWE_USCALB_20160626.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.06.26/ASO_50M_SWE_USCALB_20160626.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.07.08/ASO_50M_SWE_USCATB_20160708.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2016.07.08/ASO_50M_SWE_USCATB_20160708.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.28/ASO_50M_SWE_USCALB_20170128.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.28/ASO_50M_SWE_USCALB_20170128.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.29/ASO_50M_SWE_USCACE_20170129.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.29/ASO_50M_SWE_USCACE_20170129.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.29/ASO_50M_SWE_USCATB_20170129.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.29/ASO_50M_SWE_USCATB_20170129.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.29/ASO_50M_SWE_USCATE_20170129.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.01.29/ASO_50M_SWE_USCATE_20170129.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCACE_20170717.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCACE_20170717.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCALV_20170717.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCALV_20170717.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCARC_20170717.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCARC_20170717.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCATB_20170717.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.17/ASO_50M_SWE_USCATB_20170717.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.18/ASO_50M_SWE_USCALB_20170718.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.18/ASO_50M_SWE_USCALB_20170718.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.18/ASO_50M_SWE_USCASF_20170718.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.18/ASO_50M_SWE_USCASF_20170718.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.19/ASO_50M_SWE_USCASJ_20170719.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.19/ASO_50M_SWE_USCASJ_20170719.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.27/ASO_50M_SWE_USCACE_20170727.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.27/ASO_50M_SWE_USCACE_20170727.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.27/ASO_50M_SWE_USCATB_20170727.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.07.27/ASO_50M_SWE_USCATB_20170727.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.15/ASO_50M_SWE_USCALB_20170815.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.15/ASO_50M_SWE_USCALB_20170815.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.15/ASO_50M_SWE_USCASJ_20170815.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.15/ASO_50M_SWE_USCASJ_20170815.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.16/ASO_50M_SWE_USCACE_20170816.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.16/ASO_50M_SWE_USCACE_20170816.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.16/ASO_50M_SWE_USCATB_20170816.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2017.08.16/ASO_50M_SWE_USCATB_20170816.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.03.04/ASO_50M_SWE_USCASJ_20180304.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.03.04/ASO_50M_SWE_USCASJ_20180304.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.22/ASO_50M_SWE_USCALB_20180422.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.22/ASO_50M_SWE_USCALB_20180422.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.22/ASO_50M_SWE_USCASJ_20180422.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.22/ASO_50M_SWE_USCASJ_20180422.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCACE_20180423.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCACE_20180423.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCAJW_20180423.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCAJW_20180423.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCASF_20180423.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCASF_20180423.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCATB_20180423.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.23/ASO_50M_SWE_USCATB_20180423.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.25/ASO_50M_SWE_USCAMB_20180425.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.25/ASO_50M_SWE_USCAMB_20180425.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.26/ASO_50M_SWE_USCAKC_20180426.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.26/ASO_50M_SWE_USCAKC_20180426.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.26/ASO_50M_SWE_USCAKN_20180426.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.04.26/ASO_50M_SWE_USCAKN_20180426.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.05.28/ASO_50M_SWE_USCACE_20180528.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.05.28/ASO_50M_SWE_USCACE_20180528.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.05.28/ASO_50M_SWE_USCATB_20180528.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.05.28/ASO_50M_SWE_USCATB_20180528.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.01/ASO_50M_SWE_USCALB_20180601.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.01/ASO_50M_SWE_USCALB_20180601.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.01/ASO_50M_SWE_USCASF_20180601.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.01/ASO_50M_SWE_USCASF_20180601.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.01/ASO_50M_SWE_USCASJ_20180601.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.01/ASO_50M_SWE_USCASJ_20180601.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.02/ASO_50M_SWE_USCAJW_20180602.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2018.06.02/ASO_50M_SWE_USCAJW_20180602.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.09/ASO_50M_SWE_USCALB_20190309.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.09/ASO_50M_SWE_USCALB_20190309.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.15/ASO_50M_SWE_USCAJW_20190315.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.15/ASO_50M_SWE_USCAJW_20190315.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.16/ASO_50M_SWE_USCAKC_20190316.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.16/ASO_50M_SWE_USCAKC_20190316.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.17/ASO_50M_SWE_USCAKW_20190317.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.17/ASO_50M_SWE_USCAKW_20190317.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.17/ASO_50M_SWE_USCASF_20190317.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.17/ASO_50M_SWE_USCASF_20190317.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.24/ASO_50M_SWE_USCAKW_20190324.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.24/ASO_50M_SWE_USCAKW_20190324.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.24/ASO_50M_SWE_USCATE_20190324.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.24/ASO_50M_SWE_USCATE_20190324.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.25/ASO_50M_SWE_USCASJ_20190325.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.25/ASO_50M_SWE_USCASJ_20190325.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.26/ASO_50M_SWE_USCAKC_20190326.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.26/ASO_50M_SWE_USCAKC_20190326.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.29/ASO_50M_SWE_USCAMB_20190329.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.03.29/ASO_50M_SWE_USCAMB_20190329.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.17/ASO_50M_SWE_USCAKN_20190417.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.17/ASO_50M_SWE_USCAKN_20190417.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.17/ASO_50M_SWE_USCATE_20190417.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.17/ASO_50M_SWE_USCATE_20190417.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.18/ASO_50M_SWE_USCAKC_20190418.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.18/ASO_50M_SWE_USCAKC_20190418.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.21/ASO_50M_SWE_USCAKW_20190421.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.21/ASO_50M_SWE_USCAKW_20190421.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.27/ASO_50M_SWE_USCAKC_20190427.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.27/ASO_50M_SWE_USCAKC_20190427.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.28/ASO_50M_SWE_USCAKC_20190428.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.04.28/ASO_50M_SWE_USCAKC_20190428.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.01/ASO_50M_SWE_USCALB_20190501.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.01/ASO_50M_SWE_USCALB_20190501.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.01/ASO_50M_SWE_USCASJ_20190501.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.01/ASO_50M_SWE_USCASJ_20190501.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.02/ASO_50M_SWE_USCASF_20190502.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.02/ASO_50M_SWE_USCASF_20190502.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.03/ASO_50M_SWE_USCATE_20190503.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.05.03/ASO_50M_SWE_USCATE_20190503.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.04/ASO_50M_SWE_USCAMB_20190604.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.04/ASO_50M_SWE_USCAMB_20190604.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.05/ASO_50M_SWE_USCAJW_20190605.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.05/ASO_50M_SWE_USCAJW_20190605.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.08/ASO_50M_SWE_USCAKC_20190608.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.08/ASO_50M_SWE_USCAKC_20190608.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.09/ASO_50M_SWE_USCASF_20190609.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.09/ASO_50M_SWE_USCASF_20190609.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.11/ASO_50M_SWE_USCAKC_20190611.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.11/ASO_50M_SWE_USCAKC_20190611.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.11/ASO_50M_SWE_USCALB_20190611.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.11/ASO_50M_SWE_USCALB_20190611.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.13/ASO_50M_SWE_USCATE_20190613.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.13/ASO_50M_SWE_USCATE_20190613.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.14/ASO_50M_SWE_USCASJ_20190614.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.06.14/ASO_50M_SWE_USCASJ_20190614.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.03/ASO_50M_SWE_USCALB_20190703.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.03/ASO_50M_SWE_USCALB_20190703.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.03/ASO_50M_SWE_USCAMB_20190703.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.03/ASO_50M_SWE_USCAMB_20190703.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.04/ASO_50M_SWE_USCASF_20190704.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.04/ASO_50M_SWE_USCASF_20190704.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.04/ASO_50M_SWE_USCASJ_20190704.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.04/ASO_50M_SWE_USCASJ_20190704.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.05/ASO_50M_SWE_USCATE_20190705.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.05/ASO_50M_SWE_USCATE_20190705.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.13/ASO_50M_SWE_USCASJ_20190713.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.13/ASO_50M_SWE_USCASJ_20190713.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.14/ASO_50M_SWE_USCASF_20190714.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.14/ASO_50M_SWE_USCASF_20190714.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.15/ASO_50M_SWE_USCALB_20190715.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.15/ASO_50M_SWE_USCALB_20190715.tif.xml', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.16/ASO_50M_SWE_USCAMB_20190716.tif', 'https://n5eil01u.ecs.nsidc.org/DP1/ASO/ASO_50M_SWE.001/2019.07.16/ASO_50M_SWE_USCAMB_20190716.tif.xml']\n",
      "Downloading 262 files to /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data...\n",
      "d2hpdGVsaWdodG5pbmc6I0VhcnRoRGF0YTY5IQ==\n",
      "001/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130403.tif\n",
      "\b  [============                                                ]  20%  2.5MB/s    [========================                                    ]  40%  4.8MB/s    [====================================                        ]  60%  6.9MB/s    [================================================            ]  80%  7.9MB/s    [============================================================] 100%  9.7MB/s   \n",
      "002/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130403.tif.xml\n",
      "\b  [============================================================] 100%  953.9kB/s   \n",
      "003/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130429.tif\n",
      "\b  [============                                                ]  20%  3.9MB/s    [========================                                    ]  40%  6.1MB/s    [====================================                        ]  60%  7.7MB/s    [================================================            ]  80%  8.6MB/s   [============================================================] 100%  10.6MB/s   \n",
      "004/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130429.tif.xml\n",
      "\b  [============================================================] 100%  946.3kB/s   \n",
      "005/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130503.tif\n",
      "\b  [============                                                ]  20%  2.3MB/s    [========================                                    ]  40%  4.4MB/s    [====================================                        ]  60%  5.8MB/s    [================================================            ]  80%  6.6MB/s    [============================================================] 100%  8.2MB/s   \n",
      "006/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130503.tif.xml\n",
      "\b  [============================================================] 100%  843.9kB/s   \n",
      "007/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130525.tif\n",
      "\b  [============                                                ]  20%  4.0MB/s    [========================                                    ]  40%  5.9MB/s    [====================================                        ]  60%  7.1MB/s    [================================================            ]  80%  8.0MB/s    [============================================================] 100%  9.8MB/s   \n",
      "008/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130525.tif.xml\n",
      "\b  [============================================================] 100%  922.0kB/s   \n",
      "009/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130601.tif\n",
      "\b  [============                                                ]  20%  4.0MB/s    [========================                                    ]  40%  6.0MB/s    [====================================                        ]  60%  7.2MB/s    [================================================            ]  80%  8.0MB/s    [============================================================] 100%  9.9MB/s   \n",
      "010/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130601.tif.xml\n",
      "\b  [============================================================] 100%  896.4kB/s   \n",
      "011/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130608.tif\n",
      "\b  [============                                                ]  20%  3.6MB/s    [========================                                    ]  40%  5.4MB/s    [====================================                        ]  60%  6.6MB/s    [================================================            ]  80%  7.4MB/s    [============================================================] 100%  9.0MB/s   \n",
      "012/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20130608.tif.xml\n",
      "\b  [============================================================] 100%  971.1kB/s   \n",
      "013/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCOUB_20140320.tif\n",
      "\b  [==============================                              ]  50%  4.0MB/s    [============================================================] 100%  5.9MB/s   \n",
      "014/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCOUB_20140320.tif.xml\n",
      "\b  [============================================================] 100%  951.1kB/s   \n",
      "015/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140323.tif\n",
      "\b  [============                                                ]  20%  4.0MB/s    [========================                                    ]  40%  5.9MB/s    [====================================                        ]  60%  6.0MB/s    [================================================            ]  80%  7.4MB/s    [============================================================] 100%  9.2MB/s   \n",
      "016/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140323.tif.xml\n",
      "\b  [============================================================] 100%  995.3kB/s   \n",
      "017/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140324.tif\n",
      "\b  [====================                                        ]  33%  3.9MB/s    [========================================                    ]  67%  5.3MB/s    [============================================================] 100%  7.8MB/s   \n",
      "018/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140324.tif.xml\n",
      "\b  [============================================================] 100%  985.5kB/s   \n",
      "019/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140406.tif\n",
      "\b  [====================                                        ]  33%  3.7MB/s    [========================================                    ]  67%  5.2MB/s    [============================================================] 100%  7.5MB/s   \n",
      "020/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140406.tif.xml\n",
      "\b  [============================================================] 100%  975.1kB/s   \n",
      "021/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140407.tif\n",
      "\b  [============                                                ]  20%  4.0MB/s    [========================                                    ]  40%  4.8MB/s    [====================================                        ]  60%  6.0MB/s    [================================================            ]  80%  6.4MB/s    [============================================================] 100%  7.9MB/s   \n",
      "022/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140407.tif.xml\n",
      "\b  [============================================================] 100%  953.5kB/s   \n",
      "023/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140414.tif\n",
      "\b  [====================                                        ]  33%  3.5MB/s    [========================================                    ]  67%  5.2MB/s    [============================================================] 100%  7.1MB/s   \n",
      "024/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140414.tif.xml\n",
      "\b  [============================================================] 100%  1.0MB/s   \n",
      "025/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140420.tif\n",
      "\b  [============                                                ]  20%  4.0MB/s    [========================                                    ]  40%  5.3MB/s    [====================================                        ]  60%  6.1MB/s    [================================================            ]  80%  6.9MB/s    [============================================================] 100%  8.5MB/s   \n",
      "026/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140420.tif.xml\n",
      "\b  [============================================================] 100%  975.7kB/s   \n",
      "027/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140423.tif\n",
      "\b  [====================                                        ]  33%  2.7MB/s    [========================================                    ]  67%  4.8MB/s    [============================================================] 100%  6.6MB/s   \n",
      "028/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140423.tif.xml\n",
      "\b  [============================================================] 100%  935.3kB/s   \n",
      "029/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140428.tif\n",
      "\b  [============                                                ]  20%  3.5MB/s    [========================                                    ]  40%  4.9MB/s    [====================================                        ]  60%  5.9MB/s    [================================================            ]  80%  6.8MB/s    [============================================================] 100%  8.4MB/s   \n",
      "030/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCATB_20140428.tif.xml\n",
      "\b  [============================================================] 100%  997.8kB/s   \n",
      "031/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140429.tif\n",
      "\b  [====================                                        ]  33%  3.8MB/s    [========================================                    ]  67%  5.1MB/s    [============================================================] 100%  7.2MB/s   \n",
      "032/262: /home/rjohnson18/SWEML/data/NSMv2.0/data/ASO/SWE_Data/ASO_50M_SWE_USCAMB_20140429.tif.xml\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    short_name = 'ASO_50M_SWE'\n",
    "    version = '1'\n",
    "\n",
    "    data_tool = ASODownload(short_name, version)\n",
    "    time_start = '2013-04-02T00:00:00Z'\n",
    "    time_end = '2019-07-19T23:59:59Z'\n",
    "    \n",
    "    selected_region = data_tool.select_region()  # Call select_region on the instance\n",
    "    directory = \"SWE_Data\"\n",
    "\n",
    "    print(f\"Fetching file URLs in progress for {selected_region} from {time_start} to {time_end}\")\n",
    "    url_list = data_tool.cmr_search(time_start, time_end, data_tool.bounding_box)\n",
    "    data_tool.cmr_download(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec476660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NSMv2.0/data/Provided_Data/grid_cells_meta.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Get all SWE_csv into the input folder\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b8ea0-b7c7-4c08-8cf7-69398a1493f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASODataProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def processing_tiff(input_file, output_res):\n",
    "        try:\n",
    "            date = os.path.splitext(input_file)[0].split(\"_\")[-1]\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_folder = os.path.join(os.getcwd(), \"Processed_Data\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            output_file = os.path.join(output_folder, f\"ASO_100M_{date}.tif\")\n",
    "    \n",
    "            ds = gdal.Open(input_file)\n",
    "            if ds is None:\n",
    "                print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "                return None\n",
    "            \n",
    "            # Reproject and resample\n",
    "            gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "    \n",
    "            # Read the processed TIFF file using rasterio\n",
    "            rds = rxr.open_rasterio(output_file)\n",
    "            rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "            rds.name = \"data\"\n",
    "            df = rds.to_dataframe().reset_index()\n",
    "            return df\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_tiff_to_csv(input_folder, output_res):\n",
    "\n",
    "        curr_dir = os.getcwd()\n",
    "        folder_path = os.path.join(curr_dir, input_folder)\n",
    "        \n",
    "        # Check if the folder exists and is not empty\n",
    "        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "            print(f\"The folder '{input_folder}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        if not os.listdir(folder_path):\n",
    "            print(f\"The folder '{input_folder}' is empty.\")\n",
    "            return\n",
    "    \n",
    "        tiff_files = [filename for filename in os.listdir(folder_path) if filename.endswith(\".tif\")]\n",
    "    \n",
    "        # Create CSV files from TIFF files\n",
    "        for tiff_filename in tiff_files:\n",
    "            \n",
    "            # Open the TIFF file\n",
    "            tiff_filepath = os.path.join(folder_path, tiff_filename)\n",
    "            df = ASODataProcessing.processing_tiff(tiff_filepath, output_res)\n",
    "    \n",
    "            if df is not None:\n",
    "                # Get the date from the TIFF filename\n",
    "                date = os.path.splitext(tiff_filename)[0].split(\"_\")[-1]\n",
    "    \n",
    "                # Define the CSV filename and folder\n",
    "                csv_filename = f\"ASO_SWE_{date}.csv\"\n",
    "                csv_folder = os.path.join(curr_dir, \"Processed_Data\", \"SWE_csv\")\n",
    "                os.makedirs(csv_folder, exist_ok=True)\n",
    "                csv_filepath = os.path.join(csv_folder, csv_filename)\n",
    "    \n",
    "                # Save the DataFrame as a CSV file\n",
    "                df.to_csv(csv_filepath, index=False)\n",
    "    \n",
    "                print(f\"Converted '{tiff_filename}' to '{csv_filename}'\")\n",
    "                \n",
    "    def create_polygon(self, row):\n",
    "        return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "\n",
    "    def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "        # Import the metadata into a pandas DataFrame\n",
    "        '''\n",
    "        input_folder = f\"{HOME}/data/NSMv2.0/data/Processed_Data/SWE_csv\"\n",
    "        metadata_path = f\"{HOME}/data/NSMv2.0/data/Provided_Data/grid_cells_meta.csv\"\n",
    "        output_folder = f\"{HOME}/data/NSMv2.0/data/Processed_SWE\"\n",
    "        '''\n",
    "        try:\n",
    "            pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "        except:\n",
    "            key = \"NSMv2.0\"+metadata_path.split(\"NSMv2.0\",1)[1]        \n",
    "            S3.meta.client.download_file(BUCKET_NAME, key,metadata_path)\n",
    "            pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "\n",
    "        # Get all SWE_csv into the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "            \n",
    "    \n",
    "        # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "        pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "        pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "    \n",
    "        # Convert the DataFrame to a GeoDataFrame\n",
    "        metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "    \n",
    "        # Drop coordinates columns\n",
    "        metadata_df = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                             'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                             'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                             'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "    \n",
    "        # List all CSV files in the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    \n",
    "        for csv_file in csv_files:\n",
    "            input_aso_path = os.path.join(input_folder, csv_file)\n",
    "            output_aso_path = os.path.join(output_folder, csv_file)\n",
    "    \n",
    "            # Check if the output file already exists\n",
    "            if os.path.exists(output_aso_path):\n",
    "                print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "                continue\n",
    "    \n",
    "            # Process each CSV file\n",
    "            aso_swe_df = pd.read_csv(input_aso_path)\n",
    "    \n",
    "            # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "            geometry = [Point(xy) for xy in zip(aso_swe_df['x'], aso_swe_df['y'])]\n",
    "            aso_swe_geo = gpd.GeoDataFrame(aso_swe_df, geometry=geometry)\n",
    "\n",
    "            result = gpd.sjoin(aso_swe_geo, metadata_df, how='left', predicate='within', op = 'intersects')\n",
    "    \n",
    "            # Select specific columns for the final DataFrame\n",
    "            Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "            Final_df.rename(columns={'data': 'swe'}, inplace=True)\n",
    "    \n",
    "            # Drop rows where 'cell_id' is NaN\n",
    "            if Final_df['cell_id'].isnull().values.any():\n",
    "                Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "    \n",
    "            # Save the processed DataFrame to a CSV file\n",
    "            Final_df.to_csv(output_aso_path, index=False)\n",
    "            print(f\"Processed {csv_file}\")\n",
    "            \n",
    "    def converting_ASO_to_standardized_format(self, input_folder, output_csv):\n",
    "        \n",
    "        # Initialize an empty DataFrame to store the final transformed data\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "        # Iterate through all CSV files in the directory\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "    \n",
    "                # Extract the time frame from the filename\n",
    "                time_frame = filename.split('_')[-1].split('.')[0]\n",
    "    \n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "    \n",
    "                # Rename the 'SWE' column to the time frame for clarity\n",
    "                df = df.rename(columns={'SWE': time_frame})\n",
    "    \n",
    "                # Merge or concatenate the data into the final DataFrame\n",
    "                if final_df.empty:\n",
    "                    final_df = df\n",
    "                else:\n",
    "                    final_df = pd.merge(final_df, df, on='cell_id', how='outer')\n",
    "    \n",
    "        # Save the final transformed DataFrame to a single CSV file\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #data_processor = ASODataProcessing()\n",
    "    #folder_name = \"SWE_Data\"\n",
    "    #output_res = 0.001\n",
    "    data_processor.convert_tiff_to_csv(folder_name, output_res)\n",
    "    input_folder = f\"{HOME}/data/v2.0/Processed_Data/SWE_csv\"\n",
    "    metadata_path = f\"{HOME}/data/v2.0/Provided_Data/grid_cells_meta.csv\"\n",
    "    output_folder = f\"{HOME}/data/v2.0/Processed_SWE\"\n",
    "\n",
    "    data_processor.process_folder(input_folder, metadata_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c91c7a-d327-4dbf-8669-1e667c0e4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aso_snotel_geometry(aso_swe_file, folder_path):\n",
    "    \n",
    "    aso_file = pd.read_csv(os.path.join(folder_path, aso_swe_file))\n",
    "    aso_file.set_index('cell_id', inplace=True)\n",
    "    aso_geometry = [Point(xy) for xy in zip(aso_file['x'], aso_file['y'])]\n",
    "    aso_gdf = gpd.GeoDataFrame(aso_file, geometry=aso_geometry)\n",
    "    \n",
    "    return aso_gdf\n",
    "\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    lon1 = np.radians(lon1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    r = 6371.0\n",
    "    # Distance calculation\n",
    "    distances = r * c\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6, distance_cache=None):\n",
    "\n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    for idx, aso_row in aso_gdf.iterrows():\n",
    "        cell_id = idx\n",
    "\n",
    "        # Check if distances for this cell_id are already calculated and cached\n",
    "        if cell_id in distance_cache:\n",
    "            nearest_snotel[idx] = distance_cache[cell_id]\n",
    "        else:\n",
    "            # Calculate Haversine distances between the grid cell and all SNOTEL locations\n",
    "            distances = haversine_vectorized(\n",
    "                aso_row.geometry.y, aso_row.geometry.x,\n",
    "                snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "\n",
    "            # Store the nearest stations in the cache\n",
    "            nearest_snotel[idx] = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "            distance_cache[cell_id] = nearest_snotel[idx]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def calculate_distances_for_cell(aso_row, snotel_gdf, n=6):\n",
    "   \n",
    "    distances = haversine_vectorized(\n",
    "        aso_row.geometry.y, aso_row.geometry.x,\n",
    "        snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "    \n",
    "    nearest_sites = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "    \n",
    "    return nearest_sites\n",
    "\n",
    "def calculate_nearest_snotel_parallel(aso_gdf, snotel_gdf, n = 6, distance_cache = None):\n",
    "    \n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    with ProcessPoolExecutor(max_workers = 16) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, aso_row in aso_gdf.iterrows():\n",
    "            if idx not in distance_cache:\n",
    "                # Submit the task for parallel execution\n",
    "                futures.append(executor.submit(calculate_distances_for_cell, aso_row, snotel_gdf, n))\n",
    "            else:\n",
    "                nearest_snotel[idx] = distance_cache[idx]\n",
    "\n",
    "        # Retrieve results as they are completed\n",
    "        for future in tqdm(futures):\n",
    "            result = future.result()\n",
    "  \n",
    "            cell_id = result[0]  \n",
    "            nearest_snotel[cell_id] = result[1]\n",
    "            distance_cache[cell_id] = result[1]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def fetch_snotel_sites_for_cellids(aso_swe_files_folder_path, metadata_path, snotel_data_path):\n",
    "    \n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    #metadata_df['geometry'] = metadata_df['geometry'].apply(wkt.loads)\n",
    "    \n",
    "    def create_polygon(row):\n",
    "        return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "        \n",
    "    metadata_df = metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    metadata_df['geometry'] = metadata_df.apply(create_polygon, axis=1)\n",
    "    \n",
    "    metadata = gpd.GeoDataFrame(metadata_df, geometry='geometry')\n",
    "    snotel_data = pd.read_csv(snotel_data_path)\n",
    "\n",
    "    date_columns = snotel_data.columns[1:]\n",
    "    new_column_names = {col: pd.to_datetime(col, format='%Y-%m-%d').strftime('%Y%m%d') for col in date_columns}\n",
    "    snotel_data_f = snotel_data.rename(columns=new_column_names)\n",
    "\n",
    "    snotel_file = pd.read_csv(\"/home/vgindi/Provided_Data/ground_measures_metadata.csv\")\n",
    "    snotel_geometry = [Point(xy) for xy in zip(snotel_file['longitude'], snotel_file['latitude'])]\n",
    "    snotel_gdf = gpd.GeoDataFrame(snotel_file, geometry=snotel_geometry)\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for aso_swe_file in os.listdir(aso_swe_files_folder_path):\n",
    "\n",
    "        if os.path.isdir(os.path.join(aso_swe_files_folder_path, aso_swe_file)):\n",
    "            continue\n",
    "\n",
    "        timestamp = aso_swe_file.split('_')[-1].split('.')[0]\n",
    "        print(f\"Processing file with timestamp: {timestamp}\")\n",
    "\n",
    "        aso_gdf = load_aso_snotel_geometry(aso_swe_file, aso_swe_files_folder_path)\n",
    "        aso_swe_data = pd.read_csv(os.path.join(aso_swe_files_folder_path, aso_swe_file))\n",
    "\n",
    "        # Calculating nearest SNOTEL sites\n",
    "        nearest_snotel, distance_cache = calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6)\n",
    "        print(f\"calculated nearest snotel for file with timestamp {timestamp}\")\n",
    "\n",
    "        transposed_data = {}\n",
    "\n",
    "        if timestamp in new_column_names.values():\n",
    "            for idx, aso_row in aso_gdf.iterrows():    \n",
    "                cell_id = idx\n",
    "                station_ids = nearest_snotel[cell_id]\n",
    "                selected_snotel_data = snotel_data_f[['station_id', timestamp]].loc[snotel_data_f['station_id'].isin(station_ids)]\n",
    "                station_mapping = {old_id: f\"nearest site {i+1}\" for i, old_id in enumerate(station_ids)}\n",
    "                \n",
    "                # Rename the station IDs in the selected SNOTEL data\n",
    "                selected_snotel_data['station_id'] = selected_snotel_data['station_id'].map(station_mapping)\n",
    "\n",
    "                # Transpose and set the index correctly\n",
    "                transposed_data[cell_id] = selected_snotel_data.set_index('station_id').T\n",
    "\n",
    "            transposed_df = pd.concat(transposed_data, axis=0)\n",
    "            \n",
    "            # Reset index and rename columns\n",
    "            transposed_df = transposed_df.reset_index()\n",
    "            transposed_df.rename(columns={'level_0': 'cell_id', 'level_1': 'Date'}, inplace = True)\n",
    "            transposed_df['Date'] = pd.to_datetime(transposed_df['Date'])\n",
    "        \n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "            merged_df = pd.merge(aso_swe_data, transposed_df, how='left', on=['cell_id', 'Date'])\n",
    "        \n",
    "            final_df = pd.concat([final_df, merged_df], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "    \n",
    "            # No need to merge in this case, directly concatenate\n",
    "            final_df = pd.concat([final_df, aso_swe_data], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Merge with metadata\n",
    "    req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "    Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "    # Column renaming and ordering\n",
    "    Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "    Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                     'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                     'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                     'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "    # Save the merged data to a new file\n",
    "    output_filename = r\"/home/vgindi/Provided_Data/Merged_aso_snotel_data.csv\"\n",
    "    Result.to_csv(output_filename, index=False)\n",
    "    print(\"Processed and saved data\")\n",
    "    \n",
    "def main():\n",
    "    aso_swe_files_folder_path = r\"/home/vgindi/Processed_SWE\"\n",
    "    metadata_path = r\"/home/vgindi/Provided_Data/grid_cells_meta_idx.csv\"\n",
    "    snotel_data_path = r\"/home/vgindi/Provided_Data/ground_measures_train_featuresALLDATES.parquet\"\n",
    "    fetch_snotel_sites_for_cellids(aso_swe_files_folder_path, metadata_path, snotel_data_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c69770-62f8-4f8f-88ee-ffaac26aaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pd.read_csv(r'/home/vgindi/Provided_Data/Merged_aso_snotel_data.csv')\n",
    "Result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "    #print(signed_asset)\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "    #print(tilearray)\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)\n",
    "\n",
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/Merged_aso_nearest_sites1.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\"\"\"\n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    with rasterio.open(input_file_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.out_meta\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "                         \n",
    "        with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "def download_viirs_sca(input_dir, output_dir, shapefile_path):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(shapefile_path, 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    \n",
    "    # Iterate through each year directory in the input directory\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        if os.path.isdir(year_folder_path):\n",
    "            # Extract year from the folder name (assuming folder names like 'WY2013')\n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):        \n",
    "                if file_name.endswith('.tif'):   \n",
    "                    parts = file_name.split('_')\n",
    "                    output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                    output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                    input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                    crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                    print(f\"Processed and saved {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_directory = r\"/home/vgindi/VIIRS_Data\"\n",
    "    output_directory = r\"/home/vgindi/VIIRS_Sierras\"\n",
    "    shapefile_path = r\"/home/vgindi/Provided_Data/low_sierras_points.shp\"\n",
    "    download_viirs_sca(input_directory, output_directory, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\"\"\"\n",
    "def processing_VIIRS(input_file, output_res):\n",
    "    try:\n",
    "        # Define the output file path for TIFFs using the original file name\n",
    "        output_folder_tiff = os.path.join(\"/home/vgindi/Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(input_dir, output_res):\n",
    "    # Iterate over subdirectories in the input directory\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(\"/home/vgindi/Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/vgindi/VIIRS_Sierras\"\n",
    "    output_res = 100  # Desired resolution in meters\n",
    "    process_and_convert_viirs(input_directory, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(self, row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "    # Import the metadata into a pandas DataFrame\n",
    "    pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    # Drop coordinates columns\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                         'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                         'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                         'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        # Drop rows where 'cell_id' is NaN\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        # Save the processed DataFrame to a CSV file\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"\"\n",
    "    metadata_path = r\"\"\n",
    "    output_folder = r\"\"\n",
    "    process_folder(input_folder, metadata_path, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
